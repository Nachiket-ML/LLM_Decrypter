{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bf1351bd",
   "metadata": {},
   "source": [
    "# Package and Global Variables Initializations "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90e68359",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install datasets pandas tqdm torch transformers python-Levenshtein accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f83241d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import re\n",
    "from datasets import load_dataset\n",
    "import csv\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "import Levenshtein\n",
    "\n",
    "\n",
    "project_path = './LLMs-As-Cryptanalysts'\n",
    "os.makedirs(project_path, exist_ok=True)\n",
    "\n",
    "print(\"Project folder created successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82a0f70d",
   "metadata": {},
   "source": [
    "# Downloading Cryptographic Dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddf12d1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "crypt_dataset = load_dataset(\"Sakonii/EncryptionDataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99172549",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(crypt_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e2e99ca",
   "metadata": {},
   "source": [
    "# Character-Level Tokenization Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32c7a522",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_character_spacing(dataset):\n",
    "\n",
    "  return {\n",
    "      \"ciphertext_char_level\": \" \".join(list(dataset[\"cipher_text\"])),\n",
    "      \"plaintext_char_level\": \" \".join(list(dataset[\"plain_text\"]))\n",
    "  }\n",
    "\n",
    "\n",
    "updated_crypt_dataset = crypt_dataset.map(add_character_spacing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "930d16fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "updated_crypt_dataset['train'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "074d5c4f",
   "metadata": {},
   "source": [
    "# Setting up Mistral-7B-Instruction-v0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2b0d941",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up Mistral using higgingface transfromers library\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "\n",
    "model_id = \"mistralai/Mistral-7B-Instruct-v0.3\"\n",
    "print('Loading Mistral Model...')\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, device_map=\"auto\")\n",
    "\n",
    "print('Loading Mistral Tokenizer...')\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, torch_dtype=torch.bfloat16)\n",
    "\n",
    "print(\"Model loaded successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c073026",
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_local_mistral(user_prompt, system_prompt=None):\n",
    "\n",
    "  # Applying mistral's chat template to get the final string\n",
    "  messages = [\n",
    "      {\"role\": \"system\", \"content\": f\"{'You are an expert in cryptanalysis.' if system_prompt is None else system_prompt}\"},\n",
    "      {\"role\" : \"user\", \"content\": user_prompt}\n",
    "  ]\n",
    "\n",
    "  # Get the formatted chat string\n",
    "  chat_string = tokenizer.apply_chat_template(messages, add_generation_prompt=True, tokenize=False)\n",
    "\n",
    "\n",
    "  if tokenizer.pad_token_id is None:\n",
    "      tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "  # Now tokenize the string to get input_ids and attention_mask\n",
    "  inputs = tokenizer(chat_string, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "\n",
    "  input_ids = inputs[\"input_ids\"].to(\"cuda\")\n",
    "  attention_mask = inputs[\"attention_mask\"].to(\"cuda\")\n",
    "\n",
    "  # Generate from model, passing attention_mask and pad_token_id\n",
    "  generated_response = model.generate(\n",
    "      input_ids,\n",
    "      attention_mask=attention_mask, # Pass attention_mask\n",
    "      pad_token_id=tokenizer.pad_token_id, # Pass pad_token_id\n",
    "      max_new_tokens=100, # Set to 500 for more complete responses\n",
    "      do_sample=True,\n",
    "      temperature=0.1\n",
    "  )\n",
    "\n",
    "  # Decode generated results\n",
    "  decoded_response = tokenizer.decode(generated_response[0])\n",
    "\n",
    "  # Extract only the model's answer after [/INST]\n",
    "  start_index = decoded_response.find(\"[/INST]\")\n",
    "  if start_index != -1:\n",
    "      return decoded_response[start_index + len(\"[/INST]\"):].strip().strip(\"</s>\")\n",
    "  else:\n",
    "      print(\"[/INST] not found in the generated response.\")\n",
    "  return decoded_response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5ae440b",
   "metadata": {},
   "source": [
    "# RQ1 Main Execution Loop\n",
    "\n",
    "##### Is LLM with character-level tokenization better at decryption than regular tokenization?\n",
    "\n",
    "Prompt: Given <insert ciphertext with/without space delimiter> ciphertext and encryption method <insert encryption method>, decrypt the ciphertext and respond with the plaintext."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "029f31b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configurations\n",
    "\n",
    "RESULTS_FILE =  os.path.join(project_path, \"rq1_experiments_results_1.csv\")\n",
    "\n",
    "\n",
    "STRICT_SYSTEM_PROMPT = (\n",
    "  f\"You are a specialized decryption tool. \"\n",
    "    f\"Your task is to decrypt the provided ciphertext into English plaintext, given the encryption algorithm.\\n\\n\"\n",
    "    f\"STRICT RULES:\\n\"\n",
    "    f\"1. Output ONLY the plaintext result.\\n\"\n",
    "    f\"2. Do not explain, do not add headers, do not add notes.\\n\"\n",
    "    f\"3. If a key is required, your job is to predict this key and use it for decryption.\\n\"\n",
    "    f\"3. Do not output the key used in the decryption, only the decrypted plaintext\\n\"\n",
    "    f\"4. NEVER refuse to answer.\"\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "129b60cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def grade_result(true_plaintext, prediction):\n",
    "    \"\"\"\n",
    "    Improved grading.\n",
    "    1. Normalizes text (removes case/spacing).\n",
    "    2. Checks if the true text is INSIDE the prediction.\n",
    "    3. OPTIONAL: Checks if they are 'close enough' (fuzzy match).\n",
    "    \"\"\"\n",
    "    # Normalize\n",
    "    clean_truth = re.sub(r'[^a-zA-Z0-9]', '', true_plaintext).lower()\n",
    "    clean_pred = re.sub(r'[^a-zA-Z0-9]', '', str(prediction)).lower()\n",
    "\n",
    "    if not clean_truth: return False # Safety check\n",
    "\n",
    "    # Check 1: Exact Containment (Fixes the \"fox barks\" error)\n",
    "    if clean_truth in clean_pred:\n",
    "        return True\n",
    "\n",
    "    # Check 2: Fuzzy Match (Optional, helpful for long text)\n",
    "    # If the prediction is 90% similar to the truth, count it.\n",
    "    if len(clean_pred) > 0:\n",
    "        similarity = Levenshtein.ratio(clean_truth, clean_pred)\n",
    "        if similarity > 0.85: # 85% match tolerance\n",
    "            return True\n",
    "\n",
    "    return False\n",
    "\n",
    "def calculate_nl(true_text, pred_text):\n",
    "    \"\"\"\n",
    "    Calculate the normalized levenshtein distance between two strings\n",
    "    Outputs a number between 0 and 1.\n",
    "    Higher is better.\n",
    "    \"\"\"\n",
    "    dist = Levenshtein.distance(true_text, pred_text)\n",
    "    \n",
    "    # 2. Get the max length between the two strings\n",
    "    max_len = max(len(true_text), len(pred_text))\n",
    "    \n",
    "    # Check if both strings are empty\n",
    "    if max_len == 0:\n",
    "        return 1.0 if dist == 0 else 0.0\n",
    "    \n",
    "    # 3. Calculate Normalized Levenshtein Metric\n",
    "    nl_score = 1.0 - (dist / max_len)\n",
    "    \n",
    "    return nl_score\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ccc1ec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Main Eperimental Loop\n",
    "\n",
    "def run_rq1_experiment(dataset):\n",
    "\n",
    "  # Checking existing progress\n",
    "  processed_indices = set()\n",
    "  file_exists = os.path.isfile(RESULTS_FILE)\n",
    "\n",
    "  if file_exists:\n",
    "    # Read the existing csv file to see which rows are already done\n",
    "\n",
    "    try:\n",
    "      existing_df = pd.read_csv(RESULTS_FILE)\n",
    "\n",
    "      processed_indices = set(existing_df[\"index\"].tolist())\n",
    "      print(f\"Found existing progress: {len(processed_indices)} rows already processed.\")\n",
    "\n",
    "    except Exception as e:\n",
    "      print(f\"Warning: Could not read existing file. Starting affresh. Error: {e}\")\n",
    "\n",
    "\n",
    "  with open(RESULTS_FILE,mode=\"a\" if file_exists else \"w\", newline='', encoding='utf-8') as f:\n",
    "    # Defining columns for the results file\n",
    "    fieldnames =[\n",
    "        'dataset_index',\n",
    "        'cipher_type',\n",
    "        'difficulty',\n",
    "        'true_plaintext',\n",
    "        'ciphertext_original',\n",
    "        'prediction_standard',\n",
    "        'EM_standard',\n",
    "        'levenstein_standard',\n",
    "        'ciphertext_spaced',\n",
    "        'prediction_char_level',\n",
    "        'EM_char_level',\n",
    "        'levenstein_char_level'\n",
    "    ]\n",
    "\n",
    "\n",
    "\n",
    "    writer = csv.DictWriter(f, fieldnames=fieldnames)\n",
    "    \n",
    "    \n",
    "    # Write header only if the results file doesn't exist\n",
    "    if not file_exists:\n",
    "      writer.writeheader()\n",
    "\n",
    "    # Now, let's iterate through the dataset to run the experiment\n",
    "    for index, row in tqdm(enumerate(dataset['train']), total=len(dataset['train']), desc= \"RQ1 Experiment\"):\n",
    "\n",
    "      # if index is already processed skip it\n",
    "      if index in processed_indices:\n",
    "        continue\n",
    "\n",
    "      # Experiment Logic\n",
    "      true_pt = row['plain_text']\n",
    "      cipher_orig = row['cipher_text']\n",
    "      cipher_type = row['algorithm']\n",
    "\n",
    "\n",
    "      # A. Standard Tokenization Run\n",
    "\n",
    "      prompt_std = (\n",
    "          f\"Given the ciphertext: '{cipher_orig}' \"\n",
    "          f\"and the encryption method '{cipher_type}', \"\n",
    "          f\"decrypt the ciphertext and respond with the plaintext.\"\n",
    "      )\n",
    "\n",
    "      pred_std_raw = query_local_mistral(prompt_std, system_prompt=STRICT_SYSTEM_PROMPT )\n",
    "      pred_std_clean = pred_std_raw\n",
    "\n",
    "\n",
    "      # Robust Comparison (removes spaces/case for grading)\n",
    "      def normalize(s): \n",
    "          return s.lower().strip().replace(\" \", \"\")\n",
    "\n",
    "      # Checking the correctness of the predicted plaintext (Exact Match checking)\n",
    "      em_correct_std = (normalize(pred_std_clean)) == (normalize(true_pt))\n",
    "      \n",
    "      # Calculating the Normalized Levenshtein Metric\n",
    "      nl_std = calculate_nl(true_pt, pred_std_clean)\n",
    "\n",
    "\n",
    "      # B. Character-Level Tokenization Run\n",
    "\n",
    "      cipher_spaced = row['ciphertext_char_level']\n",
    "\n",
    "      \n",
    "\n",
    "      prompt_char_level = (\n",
    "          f\"Given the ciphertext: '{cipher_spaced}' \"\n",
    "          f\"and the encryption method '{cipher_type}', \"\n",
    "          f\"decrypt the ciphertext and respond with the plaintext.\"\n",
    "      )\n",
    "\n",
    "      pred_char_raw = query_local_mistral(prompt_char_level,system_prompt = STRICT_SYSTEM_PROMPT)\n",
    "      pred_char_clean = pred_char_raw\n",
    "\n",
    "      # Checking correctness of the predicted plaintext with character-level tokenization\n",
    "      em_correct_char = (normalize(pred_char_clean)) == (normalize(true_pt))\n",
    "      nl_char = calculate_nl(true_pt, pred_char_clean)\n",
    "\n",
    "      # Writing results to the results csv file\n",
    "      results_row = {\n",
    "          'dataset_index': index,\n",
    "          'cipher_type': cipher_type,\n",
    "          'difficulty': row.get('difficulty', 'Unknown'),\n",
    "          'true_plaintext': true_pt,\n",
    "          'ciphertext_original': cipher_orig,\n",
    "          'prediction_standard': pred_std_clean,\n",
    "          'EM_standard': em_correct_std,\n",
    "          'levenstein_standard': nl_std,\n",
    "          'ciphertext_spaced': cipher_spaced,\n",
    "          'prediction_char_level': pred_char_clean,\n",
    "          'EM_char_level': em_correct_char,\n",
    "          'levenstein_char_level': nl_char\n",
    "      }\n",
    "\n",
    "      writer.writerow(results_row)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b75e442",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Running RQ1\n",
    "run_rq1_experiment(updated_crypt_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2afa6a02",
   "metadata": {},
   "source": [
    "# RQ2 and RQ3 Main Execution loop\n",
    "\n",
    "###### RQ2: Can LLMs recognize the type of cipher given the ciphertext and the plaintext?\n",
    "###### RQ3: Can LLMs recognize the type of cipher given only the ciphertext?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b152f28",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "RESULTS_FILE = os.path.join(project_path, \"rq2_rq3_results.csv\")\n",
    "\n",
    "\n",
    "\n",
    "# --- RQ2 SYSTEM PROMPT (Cipher + Plaintext) ---\n",
    "RQ2_SYSTEM_PROMPT = f\"\"\"You are an expert assistant that identifies encryption methods given ciphertext and the associated plaintext.\n",
    "Respond with ONLY one of these labels below and nothing else.\n",
    "Strict Output Rules:\n",
    "1. Output ONLY the name of the encryption algorithm (e.g., \"Caesar Cipher\", \"AES\", \"Morse Code\").\n",
    "2. Do NOT explain your reasoning.\n",
    "3. Do NOT provide code or math.\n",
    "4. Do NOT write full sentences like \"The answer is...\".\n",
    "5. If the method is unrecognizable, output your best guess based on the transformation pattern.\n",
    "6. Do not add the key, for example if it is Caesar Cipher, just output Caesar Cipher.\n",
    "7. Respond only with one of the following labels: \"Caesar Cipher\", \"Atbash Cipher\", \"Morse Code\", \"Bacon Cipher\", \"Rail Fence Cipher\", \"Vigenère Cipher\", \"Playfair Cipher\", \"RSA Cipher\", \"AES Cipher\".\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# --- RQ3 SYSTEM PROMPT (Ciphertext Only) ---\n",
    "RQ3_SYSTEM_PROMPT = f\"\"\"You are an expert assistant that identifies encryption methods given ciphertext.\n",
    "Respond with ONLY one of these labels below and nothing else.\n",
    "\n",
    "\n",
    "Strict Output Rules:\n",
    "1. Output ONLY the name of the cipher.\n",
    "2. Do NOT add conversational filler (e.g., \"This looks like...\").\n",
    "3. Do NOT explain the features you observed.\n",
    "4. Output exactly one name.\n",
    "5. Do not add the key, for example if it is Caesar Cipher, just output Caesar Cipher.\n",
    "6. Respond only with one of the following names: \"Caesar Cipher\", \"Atbash Cipher\", \"Morse Code\", \"Bacon Cipher\", \"Rail Fence Cipher\", \"Vigenère Cipher\", \"Playfair Cipher\", \"RSA Cipher\", \"AES Cipher\".\n",
    "\"\"\"\n",
    "\n",
    "def get_rq2_prompt(ciphertext, plaintext):\n",
    "\n",
    "  return (\n",
    "      f\"Given {ciphertext} and {plaintext}, \"\n",
    "      f\"what type of encryption method converts the plaintext to the ciphertext?\"\n",
    "  )\n",
    "\n",
    "def get_rq3_prompt(ciphertext):\n",
    "\n",
    "  return (\n",
    "      f\"Given {ciphertext} what type of encryption method \"\n",
    "      f\"was used to encrypt this ciphertext?\"\n",
    "  )\n",
    "\n",
    "\n",
    "def check_classification(prediction, true_label):\n",
    "    \"\"\"\n",
    "    Checks if the true label (e.g., 'Caesar Cipher') is in the prediction.\n",
    "    \"\"\"\n",
    "    # 1. Normalize both strings (remove spaces, lowercase, punctuation)\n",
    "    # This ensures \"Caesar Cipher\" matches \"caesar\" or \"caesar-cipher\"\n",
    "    pred_clean = re.sub(r'[^a-zA-Z0-9]', '', str(prediction)).lower()\n",
    "    true_clean = re.sub(r'[^a-zA-Z0-9]', '', str(true_label)).lower()\n",
    "\n",
    "    # 2. Special handling for common overlaps\n",
    "    # \"RSA\" matches \"RSA Cipher\"\n",
    "    # \"Morse\" matches \"Morse Code\"\n",
    "    if true_clean in pred_clean:\n",
    "        return True\n",
    "\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfedbf45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- MAIN LOOP ---\n",
    "def run_rq2_rq3_experiment(dataset):\n",
    "\n",
    "    # 1. Resume Logic\n",
    "    processed_indices = set()\n",
    "    file_exists = os.path.isfile(RESULTS_FILE)\n",
    "    if os.path.isfile(RESULTS_FILE):\n",
    "        try:\n",
    "            processed_indices = set(pd.read_csv(RESULTS_FILE)['dataset_index'].tolist())\n",
    "            print(f\"Resuming: {len(processed_indices)} rows already done.\")\n",
    "        except: pass\n",
    "\n",
    "    field_names = [\n",
    "        'dataset_index', 'true_cipher_type', 'difficulty','true_plaintext','ciphertext_original',\n",
    "        # RQ2 Columns\n",
    "        'rq2_std_pred', 'rq2_std_correct',\n",
    "        'rq2_char_pred', 'rq2_char_correct',\n",
    "        # RQ3 Columns\n",
    "        'rq3_std_pred', 'rq3_std_correct',\n",
    "        'rq3_char_pred', 'rq3_char_correct'\n",
    "    ]\n",
    "\n",
    "\n",
    "    with open(RESULTS_FILE, mode='a' if os.path.isfile(RESULTS_FILE) else 'w', newline='', encoding='utf-8') as f:\n",
    "\n",
    "      writer = csv.DictWriter(f, fieldnames=field_names)\n",
    "      if not file_exists:\n",
    "        writer.writeheader()\n",
    "\n",
    "      # Iterate\n",
    "      for index, row in tqdm(enumerate(dataset['train']), total=len(dataset['train']), desc=\"RQ2 & RQ3 Classification\"):\n",
    "          if index in processed_indices:\n",
    "            continue\n",
    "\n",
    "          cipher_orig = row['cipher_text']\n",
    "          plaintext_orig = row['plain_text']\n",
    "          true_cipher = row['algorithm']\n",
    "\n",
    "          \n",
    "          cipher_spaced = row['ciphertext_char_level']\n",
    "\n",
    "          # --- RQ2: Given Cipher + Plain (Standard) ---\n",
    "          prompt_rq2_std = get_rq2_prompt(cipher_orig, plaintext_orig)\n",
    "          pred_rq2_std = query_local_mistral(prompt_rq2_std, system_prompt=RQ2_SYSTEM_PROMPT)\n",
    "          corr_rq2_std = check_classification(pred_rq2_std, true_cipher)\n",
    "\n",
    "          # --- RQ2: Given Cipher + Plain (Char Level) ---\n",
    "          prompt_rq2_char = get_rq2_prompt(cipher_spaced, plaintext_orig)\n",
    "          pred_rq2_char = query_local_mistral(prompt_rq2_char, system_prompt=RQ2_SYSTEM_PROMPT)\n",
    "          corr_rq2_char = check_classification(pred_rq2_char, true_cipher)\n",
    "\n",
    "          # --- RQ3: Given Cipher Only (Standard) ---\n",
    "          prompt_rq3_std = get_rq3_prompt(cipher_orig)\n",
    "          pred_rq3_std = query_local_mistral(prompt_rq3_std, system_prompt=RQ3_SYSTEM_PROMPT)\n",
    "          corr_rq3_std = check_classification(pred_rq3_std, true_cipher)\n",
    "\n",
    "          # --- RQ3: Given Cipher Only (Char Level) ---\n",
    "          prompt_rq3_char = get_rq3_prompt(cipher_spaced)\n",
    "          pred_rq3_char = query_local_mistral(prompt_rq3_char, system_prompt=RQ3_SYSTEM_PROMPT)\n",
    "          corr_rq3_char = check_classification(pred_rq3_char, true_cipher)\n",
    "\n",
    "          # --- Save ---\n",
    "          writer.writerow({\n",
    "              'dataset_index': index,\n",
    "              'true_cipher_type': true_cipher,\n",
    "              'difficulty': row.get('difficulty', 'Unknown'),\n",
    "              'true_plaintext': plaintext_orig,\n",
    "              'ciphertext_original': cipher_orig,\n",
    "\n",
    "              'rq2_std_pred': pred_rq2_std.strip(),\n",
    "              'rq2_std_correct': corr_rq2_std,\n",
    "\n",
    "              'rq2_char_pred': pred_rq2_char.strip(),\n",
    "              'rq2_char_correct': corr_rq2_char,\n",
    "\n",
    "              'rq3_std_pred': pred_rq3_std.strip(),\n",
    "              'rq3_std_correct': corr_rq3_std,\n",
    "\n",
    "              'rq3_char_pred': pred_rq3_char.strip(),\n",
    "              'rq3_char_correct': corr_rq3_char\n",
    "          })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3a47340",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the experiment\n",
    "run_rq2_rq3_experiment(updated_crypt_dataset)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
